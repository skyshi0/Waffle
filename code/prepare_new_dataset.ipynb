{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Goal\n",
    "1. Load data, group the original training and testing dataset together\n",
    "2. Basic statistics\n",
    "3. Transform to dataframe structure:\n",
    "\n",
    "---\n",
    "- each raw represents a single frame\n",
    "- each column represent its associated information\n",
    "    - video\n",
    "    - frame\n",
    "    - ID:video name_frame ID\n",
    "    - label\n",
    "    - resident_x_nose\n",
    "    - resident_y_nose\n",
    "    - ...\n",
    "    - resident_x_tail\n",
    "    - resident_y_tail\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import neccessary packages\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Download and unzip the data\n",
    "import os, requests, zipfile\n",
    "\n",
    "fname = 'task1_classic_classification.zip'\n",
    "url = \"https://data.caltech.edu/records/s0vdx-0k302/files/task1_classic_classification.zip?download=1\"\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "  try:\n",
    "    r = requests.get(url)\n",
    "  except requests.ConnectionError:\n",
    "    print(\"!!! Failed to download data !!!\")\n",
    "  else:\n",
    "    if r.status_code != requests.codes.ok:\n",
    "      print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "      with open(fname, \"wb\") as fid:\n",
    "        fid.write(r.content)\n",
    "else:\n",
    "  print('Data have already been downloaded!!!')\n",
    "\n",
    "if not os.path.exists('task1_classic_classification'):\n",
    "  # Unzip the file\n",
    "  with zipfile.ZipFile(fname, 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')\n",
    "\n",
    "\n",
    "# Download the script\n",
    "fname = 'calms21_convert_to_npy.py'\n",
    "url = \"https://data.caltech.edu/records/s0vdx-0k302/files/calms21_convert_to_npy.py?download=1\"\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "  try:\n",
    "    r = requests.get(url)\n",
    "  except requests.ConnectionError:\n",
    "    print(\"!!! Failed to download data !!!\")\n",
    "  else:\n",
    "    if r.status_code != requests.codes.ok:\n",
    "      print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "      with open(fname, \"wb\") as fid:\n",
    "        fid.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m test_data, _ = load_task1_data(\u001b[33m'\u001b[39m\u001b[33mdata/calms21_task1_test.npy\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;66;03m#check where you created the files in the loading notebook\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m#group training and test data together to assemble our own data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m whole_data = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'dict' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "def load_task1_data(data_path):\n",
    "  \"\"\"\n",
    "  Load data for task 1:\n",
    "      The vocaubulary tells you how to map behavior names to class ids;\n",
    "      it is the same for all sequences in this dataset.\n",
    "  \"\"\"\n",
    "  data_dict = np.load(data_path, allow_pickle=True).item()\n",
    "  dataset = data_dict['annotator-id_0']\n",
    "  # Get any sequence key.\n",
    "  sequence_id = list(data_dict['annotator-id_0'].keys())[0]\n",
    "  vocabulary = data_dict['annotator-id_0'][sequence_id]['metadata']['vocab']\n",
    "  return dataset, vocabulary\n",
    "\n",
    "\n",
    "training_data, vocab = load_task1_data('data/calms21_task1_train.npy') #check where you created the files in the loading notebook\n",
    "test_data, _ = load_task1_data('data/calms21_task1_test.npy') #check where you created the files in the loading notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences in training data: 70\n",
      "Number of sequences in test data: 19\n",
      "Number of sequences in concatenated data: 89\n"
     ]
    }
   ],
   "source": [
    "#group training and test data together to assemble our own data\n",
    "concatenated_data = {}\n",
    "concatenated_data.update(training_data)\n",
    "concatenated_data.update(test_data)\n",
    "\n",
    "print(f\"Number of sequences in training data: {len(training_data)}\")\n",
    "print(f\"Number of sequences in test data: {len(test_data)}\")\n",
    "print(f\"Number of sequences in concatenated data: {len(concatenated_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplify data in a dataframe\n",
    "\n",
    "def transform_dataset(dataset):\n",
    "\n",
    "  sequence_names = list(dataset.keys())\n",
    "\n",
    "  data = []\n",
    "\n",
    "  #columns\n",
    "  mice = ['resident','intruder']\n",
    "  coordinates = ['x', 'y']\n",
    "  bodyparts = ['nose', 'left_ear', 'right_ear', 'neck', 'left_hip', 'right_hip', 'tail_base']\n",
    "\n",
    "  print('We have ', len(sequence_names), ' sequences')\n",
    "\n",
    "  for sequence in sequence_names:\n",
    "\n",
    "      for f, frame in enumerate(dataset[sequence]['keypoints']):\n",
    "\n",
    "        tabdata = {}\n",
    "        id = sequence + str(f)\n",
    "        tabdata = {'sequence': sequence, 'frame': f, 'id': id}\n",
    "        tabdata['label'] = dataset[sequence]['annotations'][f]\n",
    "\n",
    "        # create a column for each mouse + coordinate + bodypart column\n",
    "        for m, mouse in enumerate(frame):\n",
    "\n",
    "          for c, coordinate in enumerate(mouse):\n",
    "\n",
    "            for b, c_bodypart in enumerate(coordinate):\n",
    "              column_name = mice[m] + '_' + coordinates[c] + '_' + bodyparts[b]\n",
    "              tabdata[column_name] = c_bodypart\n",
    "\n",
    "        data.append(tabdata)\n",
    "\n",
    "  print('We have ', len(data), ' frames in total in the dataset')\n",
    "  dataset_new = data\n",
    "\n",
    "  return dataset_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transform_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m whole_data = \u001b[43mtransform_dataset\u001b[49m(concatenated_data)\n\u001b[32m      2\u001b[39m df = pd.DataFrame(whole_data)\n\u001b[32m      3\u001b[39m df\n",
      "\u001b[31mNameError\u001b[39m: name 'transform_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "whole_data = transform_dataset(concatenated_data)\n",
    "df = pd.DataFrame(whole_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset to be reused\n",
    "df.to_csv('calms21_task_data.csv',header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "waffles",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
